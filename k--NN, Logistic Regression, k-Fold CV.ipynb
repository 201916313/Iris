{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7f9d179-d1b1-4385-855a-b679fa4a3e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bc55232-754c-43a7-bfe7-dd0596044b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/byeongwan/OneDrive/desktop/archive/Iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "967e8c65-94bf-4503-bd91-ed75aef411d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
       "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
       "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
       "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
       "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
       "4   5            5.0           3.6            1.4           0.2  Iris-setosa"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5a3ccb-5d26-4411-9c2b-1e6b78767778",
   "metadata": {},
   "source": [
    "# k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bf1ea8c-95b8-4b35-a4b2-224c9494b7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distances\n",
    "def euclidian(p1, p2): \n",
    "    dist = 0\n",
    "    for i in range(len(p1)):\n",
    "        dist = dist + np.square(p1[i]-p2[i])\n",
    "    dist = np.sqrt(dist)\n",
    "    return dist;\n",
    "\n",
    "def manhattan(p1, p2): \n",
    "    dist = 0\n",
    "    for i in range(len(p1)):\n",
    "        dist = dist + abs(p1[i]-p2[i])\n",
    "    return dist;\n",
    "\n",
    "def minkowski(p1, p2, q): \n",
    "    dist = 0\n",
    "    for i in range(len(p1)):\n",
    "        dist = dist + abs(p1[i]-p2[i])**q\n",
    "    dist = np.sqrt(dist)**(1/q)\n",
    "    return dist;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80478a1c-8b97-4b89-93d6-b995fb2a711b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kNN Function\n",
    "def kNN(X_train,y_train, X_test, k, dist='euclidian',q=2):\n",
    "    pred = []\n",
    "    # Adjusting the data type\n",
    "    if isinstance(X_test, np.ndarray):\n",
    "        X_test=pd.DataFrame(X_test)\n",
    "    if isinstance(X_train, np.ndarray):\n",
    "        X_train=pd.DataFrame(X_train)\n",
    "        \n",
    "    for i in range(len(X_test)):    \n",
    "        # Calculating distances for our test point\n",
    "        newdist = np.zeros(len(y_train))\n",
    "\n",
    "        if dist=='euclidian':\n",
    "            for j in range(len(y_train)):\n",
    "                newdist[j] = euclidian(X_train.iloc[j,:], X_test.iloc[i,:])\n",
    "    \n",
    "        if dist=='manhattan':\n",
    "            for j in range(len(y_train)):\n",
    "                newdist[j] = manhattan(X_train.iloc[j,:], X_test.iloc[i,:])\n",
    "    \n",
    "        if dist=='minkowski':\n",
    "            for j in range(len(y_train)):\n",
    "                newdist[j] = minkowski(X_train.iloc[j,:], X_test.iloc[i,:],q)\n",
    "\n",
    "        # Merging actual labels with calculated distances\n",
    "        newdist = np.array([newdist, y_train])\n",
    "\n",
    "        ## Finding the closest k neighbors\n",
    "        # Sorting index\n",
    "        idx = np.argsort(newdist[0,:])\n",
    "\n",
    "        # Sorting the all newdist\n",
    "        newdist = newdist[:,idx]\n",
    "        #print(newdist)\n",
    "\n",
    "        # We should count neighbor labels and take the label which has max count\n",
    "        # Define a dictionary for the counts\n",
    "        c = {'0':0,'1':0,'2':0 }\n",
    "        # Update counts in the dictionary \n",
    "        for j in range(k):\n",
    "            c[str(int(newdist[1,j]))] = c[str(int(newdist[1,j]))] + 1\n",
    "\n",
    "        key_max = max(c.keys(), key=(lambda k: c[k]))\n",
    "        pred.append(int(key_max))\n",
    "        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1327b1cd-f7ac-4256-b730-18d8b71988d4",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40bc5d62-fe68-4304-b376-d7c169ec0ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid Function \n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a51be965-fcc8-448a-a249-1f1d834a9f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost Function\n",
    "def J(h, y):\n",
    "    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18409b87-38db-4c08-984c-04a918aa6ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent Function\n",
    "def gradientdescent(X, y, lmd, alpha, num_iter, print_cost):\n",
    "\n",
    "    # select initial values zero\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    \n",
    "    costs = []  \n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        z = np.dot(X, theta)\n",
    "        h = sigmoid(z)\n",
    "        \n",
    "        # adding regularization \n",
    "        reg = lmd / y.size * theta\n",
    "        # first theta is intercept\n",
    "        # it is not regularized\n",
    "        reg[0] = 0\n",
    "        cost = J(h, y)\n",
    "        \n",
    "        gradient = np.dot(X.T, (h - y)) / y.size + reg\n",
    "        theta = theta - alpha * gradient\n",
    "    \n",
    "        if print_cost and i % 100 == 0: \n",
    "            print('Number of Iterations: ', i, 'Cost : ', cost, 'Theta: ', theta)\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "      \n",
    "    return theta, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8e70dd7-868f-4a08-97a0-decaa8242f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Function \n",
    "def predict(X_test, theta):\n",
    "    z = np.dot(X_test, theta)\n",
    "    return sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bcd5d03-9a3e-44af-94e0-8ab57bc75d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Logistic Function\n",
    "def logistic(X_train, y_train, X_test, lmd=0, alpha=0.1, num_iter=30000, print_cost = False):\n",
    "    # Adding intercept\n",
    "    intercept = np.ones((X_train.shape[0], 1))\n",
    "    X_train = np.concatenate((intercept, X_train), axis=1)\n",
    "    \n",
    "    intercept = np.ones((X_test.shape[0], 1))\n",
    "    X_test = np.concatenate((intercept, X_test), axis=1)\n",
    "\n",
    "    # one vs rest\n",
    "    u=set(y_train)\n",
    "    t=[]\n",
    "    allCosts=[]   \n",
    "    for c in u:\n",
    "        # set the labels to 0 and 1\n",
    "        ynew = np.array(y_train == c, dtype = int)\n",
    "        theta_onevsrest, costs_onevsrest = gradientdescent(X_train, ynew, lmd, alpha, num_iter, print_cost)\n",
    "        t.append(theta_onevsrest)\n",
    "        \n",
    "        # Save costs\n",
    "        allCosts.append(costs_onevsrest)\n",
    "        \n",
    "    # Calculate probabilties\n",
    "    pred_test = np.zeros((len(u),len(X_test)))\n",
    "    for i in range(len(u)):\n",
    "        pred_test[i,:] = predict(X_test,t[i])\n",
    "    \n",
    "    # Select max probability\n",
    "    prediction_test = np.argmax(pred_test, axis=0)\n",
    "    \n",
    "    # Calculate probabilties\n",
    "    pred_train = np.zeros((len(u),len(X_train)))\n",
    "    for i in range(len(u)):\n",
    "        pred_train[i,:] = predict(X_train,t[i])\n",
    "    \n",
    "    # Select max probability\n",
    "    prediction_train = np.argmax(pred_train, axis=0)\n",
    "    \n",
    "    d = {\"costs\": allCosts,\n",
    "         \"Y_prediction_test\": prediction_test, \n",
    "         \"Y_prediction_train\" : prediction_train, \n",
    "         \"learning_rate\" : alpha,\n",
    "         \"num_iterations\": num_iter,\n",
    "         \"lambda\": lmd}\n",
    "        \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de0f275-2b32-4856-954c-e6b8cf166ee9",
   "metadata": {},
   "source": [
    "# Logistic Regression from Neural Network Perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62fda5d9-9b41-4aef-a229-a766e7d4bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid Function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Select initial values zero\n",
    "def initialize_with_zeros(dim):\n",
    "    return np.zeros((dim,1)), 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e667329-4dd8-478f-895b-7af634bdc168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w, b, X, Y):\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    A = sigmoid(np.dot(w.T,X)+b) # compute activation\n",
    "    cost = -1/m*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A)) # compute cost\n",
    "    \n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    dw = 1/m*np.dot(X,(A-Y).T)\n",
    "    db = 1/m*np.sum(A-Y)\n",
    "    \n",
    "    # keep grads in a dictionary \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb00156-b3ef-4320-bcce-35a822ccdde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Cost and gradient calculation\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule\n",
    "        w = w-learning_rate*dw\n",
    "        b = b-learning_rate*db \n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "        # Print the cost every 100 training iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    # Save pameters and gradients\n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53522928-ea87-4620-aaa2-aefba43a8b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nn(w, b, X):    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities\n",
    "    A = sigmoid(np.dot(w.T,X)+b)\n",
    "        \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f12570-a19a-4cbe-94b9-dcc8e6b954cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 30000, learning_rate = 0.1, print_cost = False): \n",
    "    # pandas to numpy\n",
    "    X_train = X_train.values\n",
    "    Y_train = Y_train.values.reshape((1,Y_train.shape[0]))\n",
    "    X_test = X_test.values\n",
    "    Y_test = Y_test.values.reshape((1,Y_test.shape[0]))\n",
    "    \n",
    "    # take transpose of X\n",
    "    X_train = X_train.T\n",
    "    X_test = X_test.T\n",
    "    \n",
    "    # initialize parameters with zeros \n",
    "    w, b = initialize_with_zeros(X_train.shape[0])\n",
    "    \n",
    "    # one vs all\n",
    "    u = set(y_train)\n",
    "    param_w = []\n",
    "    param_b = []\n",
    "    allCosts = []\n",
    "    for c in u:\n",
    "        # set the labels to 0 and 1\n",
    "        ynew = np.array(y_train == c, dtype = int)\n",
    "        # Gradient descent \n",
    "        parameters, grads, costs = optimize(w, b, X_train, ynew, num_iterations, learning_rate, print_cost = print_cost)\n",
    "        \n",
    "        # Save costs\n",
    "        allCosts.append(costs)\n",
    "        \n",
    "        # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "        param_w.append(parameters[\"w\"])\n",
    "        param_b.append(parameters[\"b\"])\n",
    "    \n",
    "    # Calculate probabilties\n",
    "    pred_test = np.zeros((len(u),X_test.shape[1]))\n",
    "    for i in range(len(u)):\n",
    "        pred_test[i,:] = predict_nn(param_w[i], param_b[i], X_test)\n",
    "    \n",
    "    # Select max probability\n",
    "    Y_prediction_test = np.argmax(pred_test, axis=0)\n",
    "    \n",
    "    # Calculate probabilties\n",
    "    pred_train = np.zeros((len(u),X_train.shape[1]))\n",
    "    for i in range(len(u)):\n",
    "        pred_train[i,:] = predict_nn(param_w[i], param_b[i], X_train)\n",
    "    \n",
    "    # Select max probability\n",
    "    Y_prediction_train = np.argmax(pred_train, axis=0)\n",
    "        \n",
    "    d = {\"costs\": allCosts,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c15c3c9-72e4-453f-9dcb-33a539b0b0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_split(dataset, folds):\n",
    "        dataset_split = []\n",
    "        df_copy = dataset\n",
    "        fold_size = int(df_copy.shape[0] / folds)\n",
    "        \n",
    "        # for loop to save each fold\n",
    "        for i in range(folds):\n",
    "            fold = []\n",
    "            # while loop to add elements to the folds\n",
    "            while len(fold) < fold_size:\n",
    "                # select a random element\n",
    "                r = randrange(df_copy.shape[0])\n",
    "                # determine the index of this element \n",
    "                index = df_copy.index[r]\n",
    "                # save the randomly selected line \n",
    "                fold.append(df_copy.loc[index].values.tolist())\n",
    "                # delete the randomly selected line from\n",
    "                # dataframe not to select again\n",
    "                df_copy = df_copy.drop(index)\n",
    "            # save the fold     \n",
    "            dataset_split.append(np.asarray(fold))\n",
    "            \n",
    "        return dataset_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91e55465-80ee-49b2-af82-a2c6de70596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfoldCV(dataset, f=5, k=5, model=\"logistic\"):\n",
    "    data=cross_validation_split(dataset,f)\n",
    "    result=[]\n",
    "    # determine training and test sets \n",
    "    for i in range(f):\n",
    "        r = list(range(f))\n",
    "        r.pop(i)\n",
    "        for j in r :\n",
    "            if j == r[0]:\n",
    "                cv = data[j]\n",
    "            else:    \n",
    "                cv=np.concatenate((cv,data[j]), axis=0)\n",
    "        \n",
    "        # apply the selected model\n",
    "        # default is logistic regression\n",
    "        if model == \"logistic\":\n",
    "            # default: alpha=0.1, num_iter=30000\n",
    "            # if you change alpha or num_iter, adjust the below line         \n",
    "            c = logistic(cv[:,0:4],cv[:,4],data[i][:,0:4])\n",
    "            test = c['Y_prediction_test']\n",
    "        elif model == \"knn\":\n",
    "            test = kNN(cv[:,0:4],cv[:,4],data[i][:,0:4],k)\n",
    "            \n",
    "        # calculate accuracy    \n",
    "        acc=(test == data[i][:,4]).sum()\n",
    "        result.append(acc/len(test))\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3b209e-9850-4a88-acea-aac8470a63fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
